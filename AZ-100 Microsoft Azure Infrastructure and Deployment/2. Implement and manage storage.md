# Implement and manage storage

## Create and configure storage accounts

- Storage account is top-level account for following services:
  - **Blob Storage**
    - Object and disk storage
    - Blob storage tiers
    - Azure Search integration
    - Blob Lease for exclusive write access
      - Pass in lease id to API to modify
      - E.g. IaaS VMs lease Page Blob disks to ensure its managed by single VM
    - **Azure DataLake Storage**
      - Uses blob storage to store data
      - Big data analytics
      - Analytics interface and APIs
      - Blob storage APIs
      - Hadoop compatible access to data
      - â— GPv2 Storage accounts only
    - ***Blob types***
      - **Block Blob**
        - Composed of 100 MB blocks
        - Optimized for efficient upload
        - Insert, replace, delete, blocks
        - â— Up to 4.77TB max file size
        - â— 50.000 max blobs
      - **Append blob**
        - Can only append blocks
        - Ideal for log and audit files
        - â— 195GB max file size
      - **Page Blob**
        - Optimized for frequent read/write operations
        - Good for VM disks and databases
          - Foundation for IaaS disks
          - Stores VHD files.
          - Underlying storage for Azure SQL
        - â— Standard (HDD) / Premium (SSD) storage
        - â— 8 TB max file size
        - â— Only offered in General Purpose account types
    - ***Blob Storage Access Tiers***
      - Set on blob level.
      - Three tiers:
        1. **Hot Tier**: Frequent reads
            - Lower data access costs
            - Higher data storage costs
        2. **Cool Tier**: Accessed less frequently
            - Higher data access costs
            - Lower data storage costs
            - Optimized for data that's stored 30 days
        3. **Archive Tier**: Take hours to get data available
            - Highest data access cost
            - Lowest data storage cost
            - Optimized for data that's stored 180 days
            - â— Only supported for Block Blobs
      - Changing storage tiers incurs charges
      - â— Can't change the Storage Tier of a Blob that has snapshots
      - **Azure Blob Storage Lifecycle Management** Policies
        - E.g. configure a policy to move a blob directly to the archive storage tier X days after it's uploaded
        - Storage Account -> Blob Service -> Lifecycle Management
        - Executed daily
    - You can create snapshots on blob level and view snapshots.
    - **Write Once Read Many (WORM)**
      - Cannot be erased or modify for certain period of time.
      - Set on container level
      - Enable: Access Policy -> Add Policy -> Time-based retention *(set retention period)* / Legal hold *(attach tags)* -> Lock policy
    - ***Soft Delete***
      - Saves deleted for a specified period of time
        - Storage Account -> Blob Services -> Soft Delete
    - ***Static Website Hosting***
      - When activated it creates $web container.
      - You need to have default document and error page.
      - You can integrate Azure CDN
        - **Azure Content Delivery Network (CDN)**
          - Distributed network of cache servers
          - Provide data to user from closest source
          - Offload traffic from origin servers to CDN
            - Typically static data
          - Pricing tiers are Microsoft, Akami, Verizon (Microsoft partners)
          - Supports *HTTPS*, *large file download optimization*, *file compression*, *geo-filtering*
          - Azure CDN Core Analytics is collected and can be exported to blob storage, event hubs, Log Analytics.
        - Azure Storage blob becomes ***origin server***.
        - Azure CDN servers become ***edge servers***
        - CDN can authenticate to Blob Storage with SAS tokens to be able to read private data.
        - ***Caching rules***: On blobs you can set CDN caching rules, such as `CacheControl:max-age=86400` in blob properties.
        - ***Set up**: Two alternatives
          - Create CDN and configure against blob service.
          - Storage account -> Blob service -> CDN

      - You can have custom domain
      - You can have CORS policies
    - Integrates with ***Azure Search***
      - **Azure Search**
        - ***Structure***: Index, Fields, Documents
        - Data Load: Push data in yourself, pull data from Azure sources (SQL, Cosmos DB or blob storage)
        - Data Access: REST API, Simpleq Query, Lucene, .NET SDK
        - Features:
          - ***Fuzzy search*** handles misspelled words.
          - ***Suggestions*** from partial input.
          - ***Facets*** for categories.
          - ***Highlighting*** search tags for the results.
          - ***Tune and rank*** search results
          - ***Paging***
          - ***Geo-spatial search*** if index data has latitude and longtitude, user can get related data based on proximity
          - ***Synonyms***
        - Lexical analysis done by *Analyzers*
        - You can combine following cognitive skills in pipelines: OCR, language detection, key phrase extraction, NER, sentiment analysis, merger/split/image analysis/shaper.
      - You can provide metadata in blobs, they'll be used as fields in search index which helps categorize documents and aid with features like faceted search.
        - You can choose index content, content+metadata or just metadata.
      - Searchable blobs can be *PDF, doc/docs, xls/xlsx, ppt/pptx, msg, HTML, XML, ZIP, EML, RTF, TXT, CSV, JSON*.
  - **File Storage**
    - SMB File Shares
    - Attach to Virtual Machines as file shares
    - Integrates with Azure File Sync
      - On-prem to Azure sync with caching stratey
  - **Table Storage**
    - NoSQL Data Store
    - Scheme-less design
    - Azure Cosmos DB
  - **Queue Storage**
    - Message based
    - For building synchronous applications
  - URL format: e.g. `http://storageaccount.blob.core.windows.net`

- Storage Account Types
  - **Blob Storage**
    - *Supported services*: Blob storage
    - *Supported blob types*: Block blobs, append blobs
    - Supports blob storage access tiers (hot, cool, archive)
  - **General Purpose V1**
    - *Supported services*: Blob storage
    - â— Does not support blob storage access tiers (hot, cool, archive)
    - â— Classic deployment & ARM
    - â— Does not support ZRS (Zone Redundant Storage) replication
    - Slightly cheaper storage transaction costs, can be converted to V2.
  - **General Purpose V2**
    - Supports all latest features.
      - Including anything in General Purpose V1 and blob storage access tiers.
    - ðŸ’¡ Recommended choice when creating storage account.
    - Lower storage costs than V1
    - â— Has a changing soft limit (as of now 500 TB)
      - You can contact Azure support and request higher limits (as of now 5 PB). Same for ingress/egress limits to.

- **Storage account replication**
  - Impacts SLA
  - **Locally Redundant Storage (LRS)**
    - Three copies of data in single data center.
    - Data is spread across multiple hardware racks.
  - **Zone Redundant Storage (ZRS)**
    - Three copies of data in different availability zones in same region.
    - â— Only available for GPv2 storage accounts
  - **Geo-reduntant Storage (GRS)**
    - Three copies of data in two different data centers in two different regions.
    - â— You don't get to choose second region, they're paired regions decided by Microsoft.
    - â— Replication involves a delay.
      - RPO (recovery point objective) is typically lower than 15 minutes.
  - **Read-access Geo-reduntant Storage (RA-GRS)**
    - Same as GRS, but you get read-only access to data in secondary region.

- **Azure Storage Explorer**
  - Cross-platform client application to administer/view storage and Cosmos DB accounts.
    - Can be downloaded with Storage Account -> Open in Explorer in Portal.
    - Available in Azure portal as well (preview & simpler)
  - Can manage accounts across multiple subscriptions
  - Allows you to
    - Run storage emulator in local environment.
    - Manage SAS, CORS, access levels, meta data, files in File Share, stored procedures in Cosmos DB
    - Manage soft delete:
      - Enables recycle bin (retention period) for deleted items.
  - Connecting and authentication
    - Admin access with accont log-in
    - Limited access with account level SAS

- Azure Storage Pricing
  - Data storage cost (capacity)
  - Data operations
  - Outbound data transfer (bandwidth)
  - Geo-replication data transfer

- **Monitoring storage accounts**
  - **Activity log monitoring**
    - Management logs:
      - Role assignments
      - Regenerating Storage Account keys
      - Changing Storage Account settings
    - Not data plane logs e.g. new blob added, they're diagnostic logs
    - Activitiy Log Events: Administrative events, service health events, autoscale events, recommendations, security alerts, alerts
      - â— Stored for 19 days
      - Archival possible
        - To Storage Account
        - To Event Hub

  - **Storage analytics**
    - Type of diagnostic logs
      - Enabled in Diagnostic settings
    - â— Retention period up to 365 days.
    - Contains
      - Details of read, write, and delete operations
      - Reasons for failed requests
    - Issues can be found through monitoring or reported by users
    - Data includes: *Type of Operation*, *Success or Failure*, *Object Key*, *HTTP Status Code*, *Start Time*, *Server and E2E Latency*, *Authentication Type*, *IP Address of Caller*, *Browser Information*, *Type of Client*, *Client Operation ID*, *Server Operation ID*
    - Write blobs to blocks immediately
      - â— Can take an hour until available as flush is waited.
      - Search +/- 15 minutes and based on log metadata
    - â— 20 TB limit, independent of Storage Account total limit.
    - You can download **Microsoft Message Analyzer** and analyze logs in a good UI instead of text files.

  - **Storage analytics metrics**
    - Enabled as default
    - Integrates with Azure monitor
      - â— Data is stored 30 days.
    - Setting up alerts
    - Sends
      - **Capacity metrics**
        - For both storage accounts and individual storage services
        - Sent to Azure monitor every hour
        - Values are refreshed daily
      - **Transaction metrics**
        - Successful, failed, errors
        - Ingress/Egress of data
        - Service availability
      - **Performance metrics**: Server latency, E2E (end-to-end) latency
    - Metric dimensions: Response type, API calls, authentication type, geotype

  - **Monitoring costs**
    - ***Estimating costs***: Azure pricing calculator, Azure total cost of ownership calculator.
    - ***End of month bills***: Invoice, detailed usage CSV file
    - **Cost Management by Cloudyn**
      - Consumption, cost, performance
      - Compare costs against budget
      - Identify underutilized resoulrces
      - Access controls and alerts
      - Manage Azure, Amazon and Google cloud resources in one tool.
    - Azure Billing APIs
      - ***Non-enterprise customers***: Azure Resource RateCard API *(pricelist across different regions/currencies)*, Azure Resource Usage API
      - ***Enterprise customers***: Balance and Summary API, Usage Details API, Marketplace Store Charge API, Price Sheet API, Billing Periods API
    - In Subscription -> Cost Analysis
      - You can filter, view consumptions per resource/tags etc.
    - You can see invoices in Subscription -> Invoices
      - â— It does not show individual resources.
      - For it: Subscription -> Manage and download invoices

### Storage Account Security

- ***Management vs Data Plane***
  - Handled with RBAC in Azure AD
    - Can see storage keys: Owner, Contributor & Storage Account,Virtual Machine Contributor, Storage Account Key Operator Service
      - Reader cannot see storage keys
  - **Management Plane**
    - Administrative tasks e.g.
      - Viewing properties of storage account.
      - Deleting storage account.
      - Assigning roles to other users.
      - Modifying the configuration.
  - **Data Plane**
    - Requires access to storage account keys.
    - On blobs you can set access level to public access.
    - For authenticated access:
      - **Storage Account Keys**
        - Provides full access to the storage
        - ðŸ’¡ Best practice: Give all admins and apps same key.
          - Enables you to:
            - Regenarate secondary key.
            - Update apps to use secondary key
            - Regenerate primary key
        - Can be managed by Azure Key Vault using Powershell
          - Storage account keys are stored as Key vault secrets.
          - Azure Key Vault syncs keys with storage Account
          - Storage account keys never returned to caller.
      - **Shared Access Signatures (SAS tokens)**
        - ðŸ’¡ Better as it follows principle of the least privilege.
        - Contains permissions and start & end validity period.
          - Set read and/or write permissions.
          - Grant permissions to access only partition + row key ranges.
          - You can restrict access to IP Address(es)
          - Enforce HTTPS
        - Two types:
          - ***Service Level SAS***: Only to a single blob/file/table or queue storage.
          - ***Account Level SAS***: Applies to multiple services
        - â— It's generated by client and is not tracked by Microsoft.
          - Signature is signed with account key and ensures none of the parameters are tempered.
          - To invalidate, you'll need to regenerate storage account key used to sign SAS.
          - ðŸ’¡ Better way: **Storage Access Policies**
            - Defined on container level.
              - In portal: Containers -> Right click on container -> Access policy
            - Permissions + validity period is on server side.
            - Service level SAS only.
            - Easy to revoke by deleting the policy or changing its validity period.
        - Example url:

            | URL part | Description |
            | -------- | ----------- |
            |`https://myacount.blob.core.windows.net/container1/file1.pdf` | URL to endpoint |
            | `?sv=2017-07-29` | Rest API version |
            | `&st=2018-04-30T19%3A19%3A19Z` | Validity start time |
            | `&se=2018-05-01T19%3A19%#A19Z` | Validity end time |
            | `&sr=b` | Type of resource |
            | `&sp=r` | Permissions |
            | `&sip=168.1.5.60-168.1.5.70` | IP Address / range *(optional)* |
            | `&spr=https` | Protocol *(optional)* |
            | `&sig=pk9oGEPqYyu0K4Gutfreq9n0CJqgnjYgkEwcIEL8I0%3D` | Signature |

      - **Azure AD authentication with RBAC**
        - Available for Blob and Queue services.
        - Azure AD provides OAuth 2.0 token
        - E.g. *Storage Blob Data Contributor*, *Storage Blob Data Reader*, *Storage Queue Data Contributor*, *Storage Queue Data Reader*
        - Subscription level (for all Storage Accounts), Resource group level, Storage level or Blob container/queue level.
        - For users, groups, applications, managed service identities.
        - You register your application in AD (App Registrations)
          - You can then assign roles to your application.
          - Roles can also be assigned to **Managed Service Identity (MSI)**
            - Can set up with Azure VMs, Function Apps, VM Scale Sets
            - Credentials are injected into service instance (e.g. client id and certificate)
            - Code calls local MSI endpoint to authenticate to the resource (e.g. storage)
        - Easier management, no need to handle SAS tokens or manage keys.
      - ðŸ’¡ Azure Key Vault can manage storage account keys and generate SAS

- **Encrypt data in transit**
  - Enforced by enabling **Secure Transfer**
    - Requires HTTPS for REST API
    - Requires SMB 3.0 for Azure file service
  - When moving data e.g. between
    - Azure regions
    - On-prem to Azure storage
      - You can use Site-to-Site VPN, Point-to-Site VPN or Azure ExpressRoute.
  - The data is moved across internet.
  - ðŸ’¡ Vulnerable to Azure, good to encrypt data.
    - Configuration from outside Storage Account always requires SMB 3.0
    - SMB 2.1 does not have encryption so it's only allowed between different Azure regions.
    - *Secure transfer required* option is disabled by default.
  - SAS tokens can specify only HTTPS can be used.
  - You can also use **client side encryption**:
    - Encrypt data within the application.
    - Double encrypted as Azure storage encrypts data as default.
    - Still good idea to enforce HTTPS
      - HTTPS has built-in integrity checks to avoid network data loss
    - There are SDKS for e.g. C#, JAVA.
    - Can leverage Azure Key vault to generate and/or store keys.

- **Encrypt data in rest**
  - Every storage account has encryption enabled by default and cannot be disabled.
  - Required for many compliances e.g. privacy.
  - **Storage Service Encryption (SSE)**
    - Encrypts data before it's written
    - Decrypts data before it's read
    - Allows you to get encryption without any code
    - Applies to Standard and Premium.
    - Uses 256 bit AES.
    - Keys are managed by Microsoft by default.
    - Allows you to use your own encryption keys.
      - â— Blobs and files only.
      - â— Can only enable after the account is created.
      - â— Key vault and storage account must be in same region
        - Can be in different subscriptions
  - **Azure Disk Encryption**
    - Encrypts data disks (VHD) of VM's.
    - Handles both managed & unmanaged disks.
    - Windows VM's -> Bitlocker encryption
    - Linux VM's -> DM-crypt
    - Integrates with Key Vault to manage keys.
      - â— Key Vault must reside in same region and subscription
      - When uploading encrypted VM, you can upload encryption keys to Azure Key Vault first.
        - Good for migrating as you can use same keys as on-premises.

- **Configure network access to the storage account**
  - By default, storage accounts are accesible by all networks including internet
  - Allows you to create trust boundaries
  - Setting up networking/firewall rule
    - â— Denies all traffic by default unless any connection are explicitly opened
  - In portal: Settings -> Firewalls and Virtual Networks -> Select Network
  - You can have VNets from the same region as storage account or in a paired region.
  - **Firewall** allows you to choose IP addresses that can access VM's
    - E.g. you can set up only ExpressRoute acces.
  - When you configure Azure Storage firewalls and virtual networks

## Import and export data to Azure

- You can use portal, PowerShell, REST API, Azure CLI, or .NET Storage SDKs.
- You can upload files/folders using Azure Storage Explorer.
- You can use **AzCopy** commandline utility tool.
  - No limit to # of files in batch
  - Pattern filters to select files
  - Can continue batch after connection interruption
    - Uses internal journal file to handle it
  - Copy newer/older source files.
  - Throttle # of concurrent connections
  - Modify file name and metadata during upload.
  - Generate log file
  - Authenticate with storage account key or SAS.

- You can use physical drives
  - â— 64 bit only operating systems: Windows 8+ and Windows Server 2008+
  - Preparing the drive
    - â— NSTF only.
    - â— Drives must be encrypted using BitLocker
  - **WAImportExportTool**
    - Azure Import/Export tool
    - V1: Blob Storage, Export Jobs, V2: GP v1, GP v2
    - Allows you to copy from on-prem.
  - Importing data
    - Create import job
        1. Create storage account
        2. Prepare the drives
            - Connect disk drives to the Windows system via SATA connectors
            - Create a single NTFS volume on each drive
            - Prepare data using *WAImportExportTool*
              - Modify `dataset.csv` to include files/folders
              - Modify `driveset.csv` to include disks & encryption settings
              - Copy access key from storage account
        3. In Azure -> Create import/export job -> Import into Azure -> Select container RG -> Upload JRN (journal) file created from *WAImportExportTool* -> Choose import destination to the storage account -> Fill return shipping info
        4. Ship the drives to the Azure datacenter & update status with tracking number
    - Costs:
      - Charged: fixed price per device, return shipping costs
      - Free: for the data transfer in Azure
    - â— No SLAs on shipping
      - Estimated: 7-10 days after arrival
  - Exporting data
    1. Azure -> Create Import/Export Job -> Choose Export from Azure
    2. Select storage account and optionally containers
    3. Type shipping info
    4. Ship blank drives to Azure
    5. Azure encrypts & copies files
        - Provides recovery key for encrypted drive.
  - **Azure Databox**
    - Microsoft ships Data Box storage device
      - Each storage device has a maximum usable storage capacity of 80 TB.
    - It lets you send terabytes of data into Azure in a quick, inexpensive, and reliable way

## Azure files

- **Azure Files**
  - 99.9% SLA with availability, redundancy and disaster recovery.
  - Typical use cases:
    - Lift and shift
    - Hybrid solutions
    - Born-in-cloud applications that require shared storage are
    - Storage for cross-platform solutions
    - Any workload that currently uses a file server or NAS providing SMB access
  - REST compatible
  - **SMB**-compatible
    - File protocol over port 445
    - Can be mounted by Linux & windows & macOS compatible
    - Versions
      - **SMB 1**: Limited block sizes, chatty protocl
      - **SMB 2.1** *(Supported by Azure)*
        - No encryption
        - Better network performance than SMB 1.0
        - Group file shares, software shares
        - Supported >Windows 7, > Windows Server 2008
      - **SMB 3** *(Supported by Azure)*
        - Active-active support: Clustring with nodes
        - Transparent failover
        - RDMA support, multi channel > Lower latency
        - Enables usage of SQL and Hyper-V
        - Encryption support
        - Supported > Windows 8, > Windows Server 2012
    - Talks through port 445 and outbound connection
  - ***Create Azure File Share***
    - Multiple Azure File shares can be created under a storage account
    - Each has a name and optional quota assigned
      - Quota limits the size up to 5120 GB
    - Storage -> Files -> File Share
  - ***File access***
    - Access is via standard SMB client
    - Dialect of SMB is negotiated between the client and Azure Files upon connection
    - Encryption used if outside the Azure region or if required as part of the storage account configuration
    - SMB access utilizes the storage account name *(as user name)* and access key *(as password)*.
    - REST access can utilize SAS tokens
  - **Azure Files Snapshot**
    - Delta snapshot of a file share
    - Read-only, you can download your snapshot or mount it.
    - Azure Backup can schedule and manage snapshots
    - â— 200 snapshots per file share
    - If the file share is deleted all snapshots are also deleted
  - ***Replication options***
    - DFS-R (before it was File Replication Service)
    - `xcopy`, `robocopy`
    - Considerations: locking of files, data consistency, amount of data replicated and maintaining ACLs.
    - **Azure File Sync**
      - Enables replication from a single Azure Files share to one or more Windows based file servers
        - Windows service are in a synchronozation group.
      - Utilizes an agent deployed on each Windows Server instance that's then registered with the Storage Sync service then added to a sync group.
      - Cloud tiering
        - Least used data is moved to the cloud
          - Leaves a thumbprint on the server providing transparent access
          - Data is pulled down when access is requested.
        - Tiering is based on maintaining a certain percentage of free space.
          - Ensures around 20% is always free in file server.
        - Can be disabled
        - Is scoped to a file sync namespace.
        - â— File must be higher than 64 KB
      - ***Quality of Service (QoS)***
        - Default configuration: Server will consume maximum possible bandwith for data transfer via the storage sync service.
        - Supports network limits to be configured
        - For a VM based file server, QoS of the hypervisor can be used.
      - Considerations:
        - Avoid actions that'd cause data to be pulled down from the cloud
          - E.g. anti-virus scans, backups on-premises
        - ACL (Access Control Lists) are replicated to the cloud but are not enforced when accessed via Azure Files.
          - ðŸ’¡ Content should be restored to an IaaS VM file server to enable ACL enforcement.
        - Data can be pre-seeded via *Azure Databox* with some caveats
          - Enables pre-seeding instead of full copy over the network.
        - Be careful when combining other data replication technologies.
      - ***Workflow for replication***
        1. Deploy a storage account
        2. Deploy a *Azure File Share*
        3. Deploy *Storage Sync service*
            - Must be in same region as storage account
        4. Create a *sync group*
            - Sync group has:
              - Storage account & file share
              - Server endpoints
              - Cloud endpoints
        5. Register server
            1. Sync Service -> Registered Service -> Download **Azure File Sync Agent**
            2. Install the service and register the server
        6. Add file share into the *sync group* as server endpoint
            - ðŸ’¡ You can have only 1 cloud endpoint for the same sync group
            - You can enable/disable cloud tiering
        7. Install agent on file server
            - Supported >Windows Server 2012
            - Selected files can be skipped
        8. Register server to the *storage sync service* as server endpint
      - â— ***Scale and Limits***
        - **15** storage sync services per subscription
        - **30** sync groups per storage service
        - **1** cloud endpoint and **50** server endpoints per sync group
        - **4** TB maximum space
        - **100** GB maximum file size
        - **64** KB minimum file size to be tiered
      - ***Troubleshooting***
        - Check if TCP 445 is open for outbound traffic.
        - In metrics you can monitor for problems.
        - In Sync Services -> Sync Groups -> Group -> You can see health status and action recommendations for problems for cloud and server endpoints
        - In Event Viewer you can check FileSync events
  
## Implement Azure backup

- ***Azure backup benefits***
  - Automatic storage management
  - Unlimited scaling
  - Application-consistent backup
    - Each and every recovery point it has information for what it needs to go back to recovery point
  - Data encryption both in-rest and and in-transit
  - Unlimited data transfer
  - Long-term retention
    - No time limit

- Backs up to **Recovery Services Vault**
  - Online storage entity in Azure used to hold data such as backup copies, recovery points and backup policies.
  - Storage account is automatically created an configured
    - Comes with LRS and GRS storage account.
      - Configure in Vault -> Backup Infrastructure -> Backup Configuration
  - All backups are listed and globally controlled in *Backup Jobs*
    - You can monitor status and get reports
    - You can filter the jobs
  - **Backup policy**
    - Settings
      - Policy type
        - Azure VM
        - Azure File Share
        - SQL Server in Azure VM
      - Backup frequency
      - Retention range: daily, weekly, monthly, yearly
  - You can set inbuilt RBAC roles to vault
    - ***Backup Operator***: Manage backups but cannot remove backup, create vault, give any roles.
    - Others e.g. : *Backup Reader*, *Monitoring Reader*
  - **Backup Alerts**
    - Vault -> Backup Alerts -> Configure notifications -> Enable e-mail notifications, choose severities (critical, warning, information), select notification (per alert or hourly digest)
  - ***Enable MFA***
    - Properties -> Security settings -> Enable
    - â— Cannot be disabled when enabled once.
  - You generate **Security PIN** for critical options and Azure Backup will prompt for the pin (Properties -> Security settings)
  - When ***creating a VM back-up*** you can enable back-ups and choose a vault and policy.
    - â— VM must be in same location as recovery vault
  - To delete a vault, ensure all backups are stopped, delete backup agents/servers
  - You can configure Azure Backup Reports in Vault -> Backup Reports -> Diagnostic Settings -> Turn on diagnostics
    - You can save reports in you can archive reports in *storage accounts*, *stream to event hubs*, *send to Log Analytics*
    - After you configure a storage account for reports by using a Recovery Services vault, you can connect Azure Backup from Power BI and get a dashboard.

- ***Pricing***
  - Pay as you go storage model
  - You pay per **Protected Instance**
    - Protected instance is an application server/workload or computer that's been configured to back up to Microsoft Azure

- ***Components***

  - **Microsoft Azure IaaS VM Backup**
    - Features
      - **Policy-driven backup and retention**
        - Scheduled and on-demand backups, multiple recovery points
        - You can hwoever use to backup directly with *Backup Now*
      - **Application-consistent backup**
        - No impact on production environment and no shutdown of VMs
      - **Fabric level backup**
        - Multiple backups, centralized management, detailed tracking
    - â— New VM created by backup won't have backup policy associated with it.
    - ***Restoring and file-recovery manually***
      - Go to back-up blade for VM.
        - Two alternatives:
          1. Back Up Items -> Select backup -> Restore VM -> Select snapshot
          2. VM -> Back-up
      - Different alternatives:
        1. **Restore VM**
            - Two alternatives:
              1. Create new VM
              2. Restore disks
        2. **File recovery**
            1. Select recovery point
            2. Download script and execute on VM
                - Mounts disks from the selected recovery point
                - ðŸ’¡ If files are larger than 100 GB, restore whole VM instead
            3. Unmount disks after recovery

  - **Microsoft Azure Backup MARS Agent**
    - Called also **Recovery Services Agent**.
    - For backing up on-premises computers to Azure
      - Install back-up agent on local machine
      - Need connectivity to Microsoft Azure
    - Same configuration and control
      - Centralized management of all on-premises back-ups
    - Secure backup and recovery
      - Protected Instance is registered with Azure
    - Flow:
      1. In recovery services in portal
          1. Back-up
              - Where is your wokload running: On-premises
              - What do you want to back-up:
                - *Files and folders*, *Hyper-V*, *VMware*, *Microsoft SQL Server*, *Sharepoint*, *Exchange*, *System State*, *Bare Metal Recovery*.
          2. Backup *files and folders* and *system state*
          3. Download *Recovery Services Agent* from link provided
          4. Download credentials to enter in the workstation
          5. Transfer credentials & agents to the workstation
      2. Install the Azure backup client
          - Select a password for encryption
      3. Setup the backup
          - Click on *Schedule Backup* in agent
          - Select files/folders
          - Specify retention settings and policy
      4. Backup and restore file
          - Click on *Backup Now* in agent
          - Click on *Recover Nwo* in agent

  - **Microsoft Azure Backup Server**
  - Centralized installation
    - Can be installed on a server in Azure or on-premises
  - Free
  - Similar functionality as *Data Protection Manager (DPM)*
  - Backup a variety of instances
    - Workloads, VMWare and Hyper-V VM's, hosts, files, application workloads and barebone backups
  - Flow:
    1. Create Backup in Site Recovery Service
        - Go to Vault -> Backup
        - Get link for Azure Backup Server
    2. Install Azure Backup Server
        - Installs SQL server
    3. Configure Azure Backup Server
        1. Select management
        2. Protection Servers -> Register a server
        3. Disk Servers -> Add a disk for configuration files
        4. Create protection group
            - Add servers, workstations and workloads to the group
            - Can back-up to online and/or locally
        5. Enable disk for backup data
    4. Recover with Azure Backup Server
        - Select server -> Click on Recover Now